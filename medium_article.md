---

Cross-Detector Inconsistency in AI Text Detection: A Benchmark Study with Hybrid Evasion Techniques

Hi fellows! Hope you are great. In my subconscious, I have been considered how every AI text detection mechanism are detected. Somehow, in university life almost every course someone intends to create generative homeworks, papers or project via directly LLMs. In early days of LLMs, it is clear that everybody identifies generative nature of language used by LLMs. Nevertheless, since there were tremendous amount of human made texts in place. Only instructors with LLM detectors was able to detect whether you generate texts via AI or not. In these days, people who have early access to LLMs abused the technology just to pass course frequently. For instance,

A guy from two semester ahead suggested that I passed the humanity elective course via GPT4 humanizer plugin and it is interesting for me to pay $20 dollar on a subscription solely I mean it was just a technology released recently.

Time to time, we began to discover new models that can empowers flagship models to utilize different tasks like presentation generator or prompt enhancers on these days. However, the most dangerous ones are poisoned and humanizer models because they had a risk potentials towards humanity. Due to the intention of the humanizers it became totally unethical as purpose. For example, maybe you tend to send an email to someone, yet the person understood whether the content generated by LLM or not by itself. Their trust may dramatically decrease just by using pre generated email response. The person you sent an email may think you never pay attention what they want to ask or any interaction. It is a solid unethical concept for real life LLM usage.

---

Nowadays, there are more options for AI detection capabilities. Some depends on semantic filtering others focuses on text similarity filtering or advanced ML algorithms. However, I understood that no paid models provide full scope evasion for Turnitin AI. Just by analyzing & humanizing texts I observed many conclusions on detector models. In order to accomplish such a complex task, I ran the same university-level essays through five different AI detectors. One said it was 0% AI-generated. Another said 100%. I gave the same exact text, same exact words, wildly different verdicts. While some highlighted full AI generated content, otherwise others concluded human made. Correlation becomes crumbling variable, so I did not have a final preference technique to evade detectors.

[Figure 1: Demonstrating the Results of LLM Generated Texts via Strong Human Noise]

I implemented two machine learning mechanisms to manipulate detectors. Firstly, I was not able to preserve the content quality. When the ML model begins to swap words, it could not successfully inject correct synonyms or noises. The quality becomes nightmare, so I put SBERT sentence-level filter, TF-IDF similarity guard, Semantic similarity guard, Domain term protection and sentence level check. Each technique has its own threshold values, yet I must be strictly clear on calculations due to the nature of the machine learning model I used. Secondly, my pipeline employs a two-level optimization strategy. Thompson Sampling with Beta priors selects among variant strategies based on accumulated success/failure statistics. Q-Learning with state discretization (domain, detection level, iteration) optimizes strategy ordering. Every experiment, it happened on a real optimized essay about Silk Road currency exchange systems and it made me question everything I thought I knew about AI detection.

---

## The Experiment

Over the past several weeks, I built a pipeline that transforms AI-generated academic essays into human-sounding text to systematically test how reliable & evadable AI detectors actually are.

Here's what I did:

I used three AI models — GPT-4, Sonnet 4.6, and DeepSeek — for generation of 46 short academic essays on topics ranging from deep sea biodiversity to Weimar Republic hyperinflation (randomized topics for domains). Each essay was 400–600 words, written in a standard university style.

[Figure 2: Source model comparison — initial vs final detection scores for GPT-4, Claude, and DeepSeek generated texts. See `graph4_source_model_comparison.png`]

After that I processed them through a hybrid humanization pipeline. The pipeline works in three phases:

→ Rewrites the text using a different AI model (so GPT-4's text gets rewritten by Claude, Claude's by DeepSeek, and so on).

→ The rewrite prompt forces contractions, informal language, varied sentence lengths, and the kind of small imperfections real students make.

→ Second, it swaps out "AI-sounding" vocabulary — words like "Furthermore" become "On top of that," and "crucial" becomes "major." It also injects sentence-length variation and subtle hedging phrases. Third, a quality filter makes sure the meaning and facts are preserved. However, quality was not mostly succeed compared to academic essays.

I optimized 30 texts across three training batches (x30 academic texts), measuring detection scores before and after. On average, the pipeline reduced local detection scores by 11.6 percentage points.

[Figure 3: Training progress across 3 batches — each batch's initial score, final score, and improvement trajectory. See `graph1_training_progress.png`]

[Figure 4: Distributing Number of Batch Performances. See `graph2_improvement_distribution.png`]

[Figure 5: Strategy effectiveness — how often each humanization strategy was selected by the optimizer and its average improvement. See `graph3_strategy_effectiveness.png`]

Then I took 10 of these optimized texts and submitted them to five commercial AI detectors: GPTZero, ZeroGPT, Originality.ai, Winston AI, and QuillBot's AI detector.

I half-expected the detectors to agree. They didn't. Not even close.

---

## The Results: V1 — Initial Pipeline

Here's what each detector said about the same 10 optimized texts after V1 processing (LLM rewrite + perplexity injection):

**GPTZero:** 100% AI on every single text. Ten out of ten. No exceptions.

**Originality.ai:** Also 100% AI on every single text. Identical to GPTZero.

**Winston AI:** Average 80.8%. But wildly inconsistent — it scored one text (ocean acidification) at just 13% AI, while scoring another (refugee quota) at 100%.

**ZeroGPT:** Average 38.8%. Some texts scored 0%, others 100%. Huge variance.

**QuillBot:** Average 26.7%. Most texts classified as human or mixed.

Even at this stage, the disagreement was dramatic. But I wasn't done.

[Figure 6: External detector heatmap showing 10 texts scored by 5 detectors. Red = 100% AI, Green = 0% AI. The visual inconsistency is striking. See `graph7_external_detector_heatmap.png`]

---

## Round 2: Targeting the Weak Spots (V2)

After analyzing the V1 results, I ran a correlation analysis between my local open-source detectors and the five commercial ones. The results revealed something critical: ZeroGPT and QuillBot don't measure statistical predictability at all — they respond to *style*. Contractions, sentence length variation, informal markers. My local perplexity-based ensemble had essentially zero correlation with these classifier-based detectors.

So I added classifier-targeted optimizations to the pipeline:

→ **Contraction injection:** 26 formal→informal patterns ("do not" → "don't", "it is" → "it's"), applied to ~70% of matches.

→ **Burstiness injection:** Deliberately varying sentence lengths so the standard deviation exceeds 8 words. AI text typically has stdev < 4. Human text is bursty — short punchy sentences mixed with long complex ones.

→ **Writing imperfections:** Subtle hedging ("shows that" → "seems to show that"), unnecessary intensifiers ("important" → "really important"), informal summary prefixes ("So basically,").

Then I re-optimized all 10 texts and tested again.

The results:

**ZeroGPT V2:** Average dropped from 38.8% to **20.8%**. The refugee quota text — previously 100% AI — fell to 65.3%. Soil microbiome went from 66.4% to just 6.5%. Longevity pension hit 0%.

**QuillBot V2:** Average barely changed overall (26.7% → 24.9%), but individual texts swung wildly. Ocean acidification dropped from 20% to 5%. But Silk Road *increased* from 0% to 62%. Same pipeline, different LLM rewrite run, opposite verdict.

This non-determinism is itself a finding. If you run the same optimization pipeline twice, you get different detector scores — because the LLM rewrite is non-deterministic (temperature 0.95). If the pipeline produces different text each run, and the detectors give different verdicts each time, what exactly are they measuring?

[Figure 7: V1 vs V2 side-by-side comparison for ZeroGPT and QuillBot, showing the impact of classifier-targeted optimization. See `graph9_v1_vs_v2_comparison.png`]

The three-phase evolution tells the whole story:

| Phase | ZeroGPT | QuillBot | GPTZero | Originality |
|-------|---------|----------|---------|-------------|
| Original AI text | ~95% | ~95% | ~95% | ~95% |
| V1 (LLM rewrite + perplexity) | 38.8% | 26.7% | 100% | 100% |
| V2 (+ classifier optimization) | **20.8%** | **24.9%** | 100% | 100% |

The classifier-based detectors (ZeroGPT, QuillBot) responded to stylistic changes. The perplexity-based detectors (GPTZero, Originality.ai) didn't budge — they remained at 100% throughout all optimization rounds.

---

## The Silk Road Problem

The most dramatic example was an essay about Silk Road currency exchange systems. Here's how each detector scored it:

- **ZeroGPT:** 0% AI (fully human)
- **QuillBot:** 0% AI (fully human)
- **Winston AI:** 16% AI (mostly human)
- **GPTZero:** 100% AI (fully artificial)
- **Originality.ai:** 100% AI (fully artificial)

Same text. Five detectors. Verdicts ranging from "definitely human" to "definitely AI." If a student submitted this essay and their professor used ZeroGPT, they'd be fine. If the professor used GPTZero, they'd face an academic integrity investigation.

This isn't an edge case. Across all 10 texts, the pairwise agreement rate between detector pairs ranged from 10% to 100%. The local detectors (which I built using open-source models) agreed with each other 80–100% of the time. But they agreed with ZeroGPT only 30% of the time, and with QuillBot only 10% of the time.

[Figure 8: Pairwise detector agreement matrix. Local detectors cluster tightly (80–100%), but cross-cluster agreement drops to 10–30%. See `graph8_detector_agreement.png`]

---

## Why Do They Disagree?

After running a correlation analysis between my local detectors and the five commercial ones, I found something that explains the disagreement: there are two fundamentally different types of AI detectors, and they measure completely different things.

**Type 1: Perplexity-based detectors.** These tools — including Binoculars (the strongest component of my local ensemble), GLTR, and Winston AI — analyze how *predictable* each word is. AI text tends to be very predictable: each word is the statistically most likely next word given the context. Human text is messier — we use unexpected words, unusual phrasings, and unpredictable structures. These detectors are mathematically grounded and hard to fool without degrading the text.

**Type 2: Classifier-based detectors.** ZeroGPT and QuillBot appear to use machine learning classifiers trained on features like: Does the text use contractions? How varied are the sentence lengths? Are there informal markers? These classifiers learned that AI text tends to be formal, uniform in structure, and devoid of contractions. When you add contractions and vary sentence lengths, these detectors get confused.

My correlation analysis (using the final V2-optimized texts) proved this split. Binoculars correlated with ZeroGPT at r=-0.65 (p=0.041) — a *statistically significant negative* correlation. When Binoculars says "more AI," ZeroGPT actually says "less AI." They're measuring opposite things. Meanwhile, none of the other detector pairs showed any significant correlation at all — Binoculars vs Winston was r=+0.24 (p=0.51), and the ensemble vs QuillBot was r=-0.27 (p=0.44). No meaningful relationship.

The only statistically significant link across the entire 7-detector matrix is a *negative* one. That's not just disagreement — it's opposition.

[Figure 9: Correlation heatmap between local and external detectors. Binoculars shows a significant negative correlation with ZeroGPT (r=-0.65). All other cross-detector correlations are statistically insignificant. See `graph5_correlation_heatmap.png`]

[Figure 10: Scatter plots showing ensemble score vs each external detector with regression lines. The negative trend for ZeroGPT and flat/noisy relationships for others visualize the correlation split. See `graph6_scatter_correlations.png`]

---

## What This Means for You

**If you're a student:** Your genuinely human-written work might get flagged by one detector and cleared by another. A 2025 CDT survey found that 43% of US teachers use AI detectors. If your professor runs your essay through GPTZero, you might face scrutiny that wouldn't exist if they'd used ZeroGPT. This isn't about whether you used AI — it's about which detector your institution chose.

**If you're a teacher or professor:** Relying on a single AI detector is risky. The disagreement between tools isn't minor — it's fundamental. Two detectors can give diametrically opposite verdicts on the same text. Using AI detection results as the sole basis for academic integrity decisions is, at best, unreliable.

**If you're a publisher or employer:** There is no gold standard for AI detection. Even the most confident detectors (GPTZero and Originality.ai at 100%) disagree completely with other tools. The technology is useful as one signal among many, but it's not a definitive answer.

---

## The Bigger Question

My research started as a technical challenge: can you build a pipeline that reduces AI detection scores? The answer is yes, with significant caveats. My pipeline reduced local detection scores by an average of 11.6 percentage points across 30 texts. After two rounds of optimization — first targeting statistical patterns (V1), then classifier-specific features like contractions and burstiness (V2) — ZeroGPT scores dropped from ~95% to 20.8% average. But against perplexity-based detectors like GPTZero, every single text still scored 100%.

But the more interesting finding isn't about evasion — it's about detection itself. If five detectors can't agree on whether a text is AI-generated, what does "AI-generated" even mean in a detection context?

Maybe the answer isn't building better detectors. Maybe it's rethinking our approach entirely. Watermarking (embedding invisible signals in AI output), provenance tracking (recording the origin of text), or simply teaching people to use AI as a tool rather than a shortcut — these approaches don't depend on the impossible task of reliably distinguishing human from machine writing.

The AI detection arms race is accelerating, but based on my findings, it's built on a shaky foundation. The same text can be "definitely human" or "definitely AI" depending on which tool you ask.

And that should concern everyone.

---

*This research involved 46 AI-generated texts, 30 optimization runs, 10 external benchmark tests (V1 + V2), 9 analysis graphs, and a correlation study across 7 detectors. For full technical details including methodology, statistical analysis, and the complete pipeline architecture, see the accompanying research paper.*

*Onurcan Genc is a cybersecurity researcher, CVE contributor, and Computer Technology and Information Systems student at Bilkent University, Ankara, Turkey.*

**Tags:** AI Detection, GPTZero, Machine Learning, Academic Integrity, Natural Language Processing